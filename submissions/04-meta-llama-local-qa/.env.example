# Ollama Configuration (local inference)
OLLAMA_URL=http://localhost:11434
LLAMA_MODEL=llama3.2
