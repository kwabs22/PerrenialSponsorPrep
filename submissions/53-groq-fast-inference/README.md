# Groq Ultra-Fast Inference

**Showcases:** Groq LPU for lightning-fast LLM inference

## What it does
Experience the fastest LLM inference available with Groq's Language Processing Units (LPUs). Run Llama, Mixtral, and other models at unprecedented speeds - perfect for real-time applications.

## Latest Feature (December 2024)
- **LPU Inference** - 10x faster than GPU inference
- **Llama 3.2 Support** - Latest Meta models
- **Mixtral 8x7B** - MoE architecture support
- **JSON Mode** - Structured output generation

## Setup

1. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

2. Copy `.env.example` to `.env` and add your API key:
   ```bash
   cp .env.example .env
   ```

3. Run the demo:
   ```bash
   python main.py
   ```

## API Reference
- [Groq API Docs](https://console.groq.com/docs/quickstart)
- [Model Library](https://console.groq.com/docs/models)
